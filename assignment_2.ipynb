{
 "cells": [
  {
   "metadata": {
    "id": "6S7DHV3pqERC"
   },
   "cell_type": "markdown",
   "source": "### Importing Libraries"
  },
  {
   "metadata": {
    "id": "XIJsXYmUp8qO",
    "ExecuteTime": {
     "end_time": "2024-10-13T09:02:13.625482Z",
     "start_time": "2024-10-13T09:02:10.577591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import random\n",
    "\n",
    "#import any other library you need below this line"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "id": "UmOBtE8PqH4w"
   },
   "cell_type": "markdown",
   "source": [
    "### Loading data\n",
    "\n",
    "Upload the data in zip format to Colab. Then run the cell below."
   ]
  },
  {
   "metadata": {
    "id": "VMLW_lgTqRcL"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!unzip data.zip"
  },
  {
   "metadata": {
    "id": "3UoM-TMIqTna"
   },
   "cell_type": "markdown",
   "source": "### Defining the Dataset Class"
  },
  {
   "metadata": {
    "id": "6awOO200qYSZ",
    "ExecuteTime": {
     "end_time": "2024-10-13T09:02:31.374816Z",
     "start_time": "2024-10-13T09:02:31.365254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms.transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# import any other libraries you need below this line\n",
    "import torchvision.transforms.functional as v1\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "\n",
    "class Cell_data(Dataset):\n",
    "    def __init__(self, data_dir, size, train='True', train_test_split=0.8, augment_data=True):\n",
    "        ##########################inputs##################################\n",
    "        # data_dir(string) - directory of the data#########################\n",
    "        # size(int) - size of the images you want to use###################\n",
    "        # train(boolean) - train data or test data#########################\n",
    "        # train_test_split(float) - the portion of the data for training###\n",
    "        # augment_data(boolean) - use data augmentation or not#############\n",
    "        super(Cell_data, self).__init__()\n",
    "        # todo\n",
    "        # initialize the data class\n",
    "        self.data_dir = data_dir\n",
    "        self.size = size\n",
    "        self.train = train\n",
    "        self.train_test_split = train_test_split\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        # read images and masks\n",
    "        image_path = os.path.join(data_dir, 'scans')\n",
    "        mask_path = os.path.join(data_dir, 'labels')\n",
    "        \n",
    "        images = sorted([os.path.join(image_path, file) for file in os.listdir(image_path) if file.endswith('.bmp')])\n",
    "        masks = sorted([os.path.join(mask_path, file) for file in os.listdir(mask_path) if file.endswith('.bmp')])\n",
    "\n",
    "        # split train set & test set\n",
    "        idx = int(train_test_split * len(images))\n",
    "        if train:\n",
    "            self.images = images[:idx]\n",
    "            self.masks = masks[:idx]\n",
    "        else:\n",
    "            self.images = images[idx:]\n",
    "            self.masks = masks[idx:]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # load image and mask from index idx of your data\n",
    "        image_path = self.images[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        image = self.load_image(image_path)\n",
    "        mask = self.load_mask(mask_path)\n",
    "\n",
    "        if not self.train:\n",
    "            return image, mask\n",
    "\n",
    "        # data augmentation part\n",
    "        # reference: https://pytorch.org/vision/main/auto_examples/transforms\n",
    "        # /plot_transforms_illustrations.html#sphx-glr-auto-examples-transforms-plot-transforms-illustrations-py\n",
    "        if not self.augment_data:\n",
    "            augment_mode = np.random.randint(0, 5)\n",
    "            if augment_mode == 0:\n",
    "                print(\"flip vertically\")\n",
    "                # flip image vertically\n",
    "                image = v1.vflip(image)\n",
    "                mask = v1.vflip(mask)\n",
    "            elif augment_mode == 1:\n",
    "                print(\"flip horizontally\")\n",
    "                # flip image horizontally\n",
    "                image = v1.hflip(image)\n",
    "                mask = v1.hflip(mask)\n",
    "            elif augment_mode == 2:\n",
    "                print(\"zoom image\")\n",
    "                # zoom image\n",
    "                image = v2.RandomResizedCrop(size=(self.size, self.size))(image)\n",
    "                mask = v2.RandomResizedCrop(size=(self.size, self.size))(image)\n",
    "            elif augment_mode == 3:\n",
    "                print(\"rotate image\")\n",
    "                # rotate image\n",
    "                image = v2.RandomRotation((0, 360))(image)\n",
    "                mask = v2.RandomRotation((0, 360))(mask)\n",
    "            elif augment_mode == 4:\n",
    "                print(\"non-rigid transformation\")\n",
    "                # Convert image and mask tensors to PIL images\n",
    "                image_pil = v1.to_pil_image(image)\n",
    "                mask_pil = v1.to_pil_image(mask)\n",
    "\n",
    "                # Apply ElasticTransform\n",
    "                elastic_transform = v2.ElasticTransform(alpha=50.0)\n",
    "                image_pil = elastic_transform(image_pil)\n",
    "                mask_pil = elastic_transform(mask_pil)\n",
    "\n",
    "                # Convert back to tensors\n",
    "                image = v1.to_tensor(image_pil)\n",
    "                mask = v1.to_tensor(mask_pil)\n",
    "            else:\n",
    "                print(\"gamma correction\")\n",
    "\n",
    "        # todo\n",
    "        # return image and mask in tensors\n",
    "        return image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    # Helper function to load images, given file path, return a tensor\n",
    "    def load_image(self, path):\n",
    "        image = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (self.size, self.size))\n",
    "        image = cv2.normalize(image, image, alpha=0.0, beta=1.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        image_tensor = torch.from_numpy(image).unsqueeze(0)  # convert to tensor\n",
    "        return image_tensor\n",
    "\n",
    "    # Helper function to load masks, given file path, return a tensor\n",
    "    def load_mask(self, path):\n",
    "        image = cv2.resize(cv2.imread(path, cv2.IMREAD_GRAYSCALE), (self.size, self.size))\n",
    "        image_tensor = torch.from_numpy(image.astype(np.float32)).unsqueeze(0)  # convert to tensor\n",
    "        return image_tensor\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "id": "Jo6kRDASsc5t"
   },
   "cell_type": "markdown",
   "source": [
    "### Define the Model\n",
    "1. Define the Convolution blocks\n",
    "2. Define the down path\n",
    "3. Define the up path\n",
    "4. combine the down and up path to get the final model"
   ]
  },
  {
   "metadata": {
    "id": "qcOEN68psaxF",
    "ExecuteTime": {
     "end_time": "2024-10-13T09:02:35.423763Z",
     "start_time": "2024-10-13T09:02:35.414820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# import any other libraries you need below this line\n",
    "import torchvision.transforms.functional as v1\n",
    "\n",
    "class twoConvBlock(nn.Module):\n",
    "    \"\"\"Part 1  The Convolutional blocks\"\"\"\n",
    "\n",
    "    # initialize the block\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(twoConvBlock, self).__init__()\n",
    "        self.doubleConvBlock = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, output_channel, kernel_size=3, padding=1),  # 3 × 3 un-padded convolution layer\n",
    "            nn.ReLU(inplace=True),  # ReLU\n",
    "            nn.Conv2d(output_channel, output_channel, kernel_size=3, padding=1),  # 3 × 3 un-padded convolution layer\n",
    "            nn.BatchNorm2d(output_channel),  # Batch normalization layer\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # implement the forward path\n",
    "        return self.doubleConvBlock(x)\n",
    "\n",
    "\n",
    "class downStep(nn.Module):\n",
    "    \"\"\"Part 2  The Contracting path\"\"\"\n",
    "\n",
    "    # initialize the down path\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(downStep, self).__init__()\n",
    "        self.convBlock = twoConvBlock(input_channel, output_channel)  # 2 conv blocks\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size=2)  # 2 x 2 max pool\n",
    "\n",
    "    def forward(self, x):\n",
    "        # implement the forward path\n",
    "        x = self.convBlock(x)\n",
    "        x_maxPool = self.maxPool(x)\n",
    "        return x, x_maxPool\n",
    "\n",
    "\n",
    "class upStep(nn.Module):\n",
    "    \"\"\"Part 3  The Expansive path\"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, output_channel):\n",
    "        super(upStep, self).__init__()\n",
    "        # initialize the up path\n",
    "        self.upConv = nn.ConvTranspose2d(input_channel, output_channel, kernel_size=2,\n",
    "                                         stride=2)  # transpose convolutions\n",
    "        self.convBlock = twoConvBlock(output_channel * 2, output_channel)  #\n",
    "\n",
    "    def forward(self, x, skip_connection):\n",
    "        # implement the forward path\n",
    "        x = self.upConv(x)\n",
    "\n",
    "        # process crop and copy\n",
    "        # c,h,w\n",
    "        diffY = skip_connection.size()[2] - x.size()[2]\n",
    "        diffX = skip_connection.size()[3] - x.size()[3]\n",
    "        # reference : https://github.com/milesial/Pytorch-UNet/blob/67bf11b4db4c5f2891bd7e8e7f58bcde8ee2d2db/unet/unet_parts.py\n",
    "        skip_connection = F.pad(skip_connection, [-diffX // 2, -(diffX - diffX // 2),\n",
    "                                                  -diffY // 2, -(diffY - diffY // 2)])\n",
    "        target_size = (x.size(2), x.size(3))\n",
    "        skip_connection = v1.center_crop(skip_connection, output_size=target_size)\n",
    "        x = torch.cat([x, skip_connection], dim=1)\n",
    "        new_x = self.convBlock(x)\n",
    "        return new_x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        # initialize the complete model\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Contracting\n",
    "        self.inc = downStep(n_channels, 64)\n",
    "        self.down1 = downStep(64, 128)\n",
    "        self.down2 = downStep(128, 256)\n",
    "        self.down3 = downStep(256, 512)\n",
    "\n",
    "        # Bottom, no max pooling\n",
    "        self.bot = twoConvBlock(512, 1024)\n",
    "\n",
    "        # Expansive\n",
    "        self.up1 = upStep(1024, 512)\n",
    "        self.up2 = upStep(512, 256)\n",
    "        self.up3 = upStep(256, 128)\n",
    "        self.up4 = upStep(128, 64)\n",
    "\n",
    "        # Output\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # implement the forward path\n",
    "        x1, x1_maxpool = self.inc(x)\n",
    "        x2, x2_maxpool = self.down1(x1_maxpool)\n",
    "        x3, x3_maxpool = self.down2(x2_maxpool)\n",
    "        x4, x4_maxpool = self.down3(x3_maxpool)\n",
    "\n",
    "        x_bot = self.bot(x4_maxpool)\n",
    "\n",
    "        x = self.up1(x_bot, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "\n",
    "        x_out = self.outc(x)\n",
    "\n",
    "        return x_out\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T09:02:36.752046Z",
     "start_time": "2024-10-13T09:02:36.726403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.cuda.is_available())  # Should return True if CUDA is installed correctly\n",
    "print(torch.cuda.device_count())  # Number of GPUs available"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "id": "P5-0LnQItdth"
   },
   "cell_type": "markdown",
   "source": "### Training"
  },
  {
   "metadata": {
    "id": "NmFg17HktfBW",
    "ExecuteTime": {
     "end_time": "2024-10-13T09:04:36.072710200Z",
     "start_time": "2024-10-13T09:02:40.299007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Paramteres\n",
    "\n",
    "#learning rate\n",
    "lr = 1e-2\n",
    "\n",
    "#number of training epochs\n",
    "epoch_n = 20\n",
    "\n",
    "#input image-mask size\n",
    "image_size = 572\n",
    "#root directory of project\n",
    "root_dir = os.getcwd()\n",
    "\n",
    "#training batch size\n",
    "batch_size = 32\n",
    "\n",
    "#use checkpoint model for training\n",
    "load = False\n",
    "\n",
    "#use GPU for training\n",
    "gpu = True\n",
    "\n",
    "data_dir = os.path.join(root_dir, 'data/cells')\n",
    "\n",
    "\n",
    "trainset = Cell_data(data_dir = data_dir, size = image_size)\n",
    "trainloader = DataLoader(trainset, batch_size = batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "testset = Cell_data(data_dir = data_dir, size = image_size, train = False, num_workers=4)\n",
    "testloader = DataLoader(testset, batch_size = batch_size)\n",
    "\n",
    "device = torch.device('cuda:0' if gpu else 'cpu')\n",
    "\n",
    "model = UNet(n_channels=1, n_classes=batch_size).to('cuda:0').to(device)\n",
    "\n",
    "if load:\n",
    "  print('loading model')\n",
    "  model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.0005)\n",
    "\n",
    "model.train()\n",
    "for e in range(epoch_n):\n",
    "  epoch_loss = 0\n",
    "  model.train()\n",
    "  for i, data in enumerate(trainloader):\n",
    "    image, label = data\n",
    "\n",
    "    image = image.to(device)\n",
    "    label = label.squeeze(1).long().to(device)\n",
    "\n",
    "    pred = model(image)\n",
    "\n",
    "    crop_x = (label.shape[1] - pred.shape[2]) // 2\n",
    "    crop_y = (label.shape[2] - pred.shape[3]) // 2\n",
    "\n",
    "    label = label[:, crop_x: label.shape[1] - crop_x, crop_y: label.shape[2] - crop_y]\n",
    "    \n",
    "    loss = criterion(pred, label)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    print('batch %d --- Loss: %.4f' % (i, loss.item() / batch_size), flush=True)\n",
    "  print('Epoch %d / %d --- Loss: %.4f' % (e + 1, epoch_n, epoch_loss / trainset.__len__()), flush=True)\n",
    "\n",
    "  torch.save(model.state_dict(), 'checkpoint.pt')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "  model.eval()\n",
    "\n",
    "  total = 0\n",
    "  correct = 0\n",
    "  total_loss = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(testloader):\n",
    "      image, label = data\n",
    "\n",
    "      image = image.to(device)\n",
    "      label = label.long().to(device)\n",
    "\n",
    "      pred = model(image)\n",
    "      crop_x = (label.shape[1] - pred.shape[2]) // 2\n",
    "      crop_y = (label.shape[2] - pred.shape[3]) // 2\n",
    "\n",
    "      label = label[:, crop_x: label.shape[1] - crop_x, crop_y: label.shape[2] - crop_y]\n",
    "\n",
    "      loss = criterion(pred, label)\n",
    "      total_loss += loss.item()\n",
    "\n",
    "      _, pred_labels = torch.max(pred, dim = 1)\n",
    "\n",
    "      total += label.shape[0] * label.shape[1] * label.shape[2]\n",
    "      correct += (pred_labels == label).sum().item()\n",
    "\n",
    "    print('Accuracy: %.4f ---- Loss: %.4f' % (correct / total, total_loss / testset.__len__()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT-64s70tyBw"
   },
   "source": [
    "### Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ko9zFomNuCfC"
   },
   "source": [
    "model.eval()\n",
    "\n",
    "\n",
    "output_masks = []\n",
    "output_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  for i in range(testset.__len__()):\n",
    "    image, labels = testset.__getitem__(i)\n",
    "    \n",
    "    input_image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    pred = model(input_image)\n",
    "\n",
    "    output_mask = torch.max(pred, dim = 1)[1].cpu().squeeze(0).numpy()\n",
    "\n",
    "    crop_x = (labels.shape[0] - output_mask.shape[0]) // 2\n",
    "    crop_y = (labels.shape[1] - output_mask.shape[1]) // 2\n",
    "    labels = labels[crop_x: labels.shape[0] - crop_x, crop_y: labels.shape[1] - crop_y].numpy()\n",
    "    \n",
    "    output_masks.append(output_mask)\n",
    "    output_labels.append(labels)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2OrV7k1GuFSA"
   },
   "source": [
    "fig, axes = plt.subplots(testset.__len__(), 2, figsize = (20, 20))\n",
    "\n",
    "for i in range(testset.__len__()):\n",
    "  axes[i, 0].imshow(output_labels[i])\n",
    "  axes[i, 0].axis('off')\n",
    "  axes[i, 1].imshow(output_masks[i])\n",
    "  axes[i, 1].axis('off')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "UNet_FW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "system-python",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
